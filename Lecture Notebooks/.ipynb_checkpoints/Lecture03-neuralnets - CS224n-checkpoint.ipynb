{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS224n: NLP with Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 3: Word Window Classification, Neural Networks and Calculus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification review/intro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Our learned line between red and green is our classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax classifier\n",
    "$$ p(y|x) = \\frac{exp(W_{y} \\cdot x)}{\\sum_{c=1}^C exp(W_{c} \\cdot x)} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2 steps:\n",
    "1. the $y^{th}$ row of $W$ is for the $y^{th}$ class  \n",
    "Mutilpying it by $x$ gives us a score :\n",
    "2. Then we apply the softmax function to get a probability distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Objective:**\n",
    "minimize our negative log-probability $ -log p(y|x) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-entropy loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* $p = $  true probability distribution\n",
    "* $q = $  computed probability distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross-entropy:\n",
    "$$ H(p,q) = - \\sum_{c=1}^C p(c) \\, log \\, q(c) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross-entropy loss:\n",
    "$$ J(\\theta) = \\frac{1}{N} \\sum_{i=1}^N -log \\left( \\frac{e^{f_{y_i}}} {\\sum_{c=1}^C e^{f_c}} \\right) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural nets intro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Logistic regression, SVM, NaÃ¯ve Bayes are all simple classifiers:  \n",
    "they just draw a line in some high-dimensional space\n",
    "\n",
    "* But oftentimes we'd like a more sophisticated classifier, especially on natural data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLP Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We're simultaneously changing both the weights, and the words vector representations as we're learning\n",
    "* We're optimising both of them at once\n",
    "* Representation Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can interpret the word embeddings as the first hidden layer of a network that takes as inputs the 1-hot encoded vectors of the words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Biological Neuron analogy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dendrites -> Enough Signal -> Body of the Neuron -> Axon -> Dendrites of other Neurons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A little bit of Neural Net History"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 50s: Perceptron\n",
    "* 80s-90s: 1 hidden layer neural net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We need to indtroduce a non-linearity for our neural net to learn something interesting, otherwise we just get another linear function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Named Entity Recognition (NER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task = find the names of things, and classify them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Possible usages:\n",
    "* track the names of companies and people\n",
    "* answers of questions often are named entities\n",
    "* information often is just an association of 2 named entities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Difficulties"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Hard to determine the boundaries of an entity\n",
    "* Hard to know if sth is an entity\n",
    "* Hard to know the class of a new entity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary true vs corrupted word window clasification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Idea:**  \n",
    "Classify a word in its context window"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could design a classifier that classifies the middle word according to the context window words: this was, the order information would be preserved \n",
    "\n",
    "(as opposite to taking the sum vector of all the context words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NER of center-of-window word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to have a system that returns a score if there is a Location name in the middle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Concatenate the context window words' vector representations\n",
    "2. Pass it through a first non-linearity\n",
    "3. Multiply it by a big $U^$ matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Putting our extra hidden layer allows us to calculate **non-linear** things, from our input vector, such as:\n",
    "* The first word is 'museum'\n",
    "* AND the 2nd word is 'in'  \n",
    "\n",
    "Then, our 3rd word (here, our center word), is a Location name.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix calculus intro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "## Big section Title"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sub-section Title"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* King - Man ~= Idea of Kingship without the Man Part\n",
    "* +Woman = add Woman Idea to it\n",
    "* => Get Queen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot words on a scatter plot:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for word, (x,y) in zip(words, twodim):\n",
    "    plt.text(x+0.05, y+0.05, word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2vec : Vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 2 big matrices:\n",
    "* 1 that represents every outside word's vector $U$\n",
    "* 1 which represents every center word's vector $V$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$U = \\begin{bmatrix}[outside\\,word\\,vector\\,1]\n",
    "\\\\ \\vdots\n",
    "\\\\ [outside\\,word\\,vector\\,n] \\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$V = \\begin{bmatrix}[center\\,word\\,vector\\,1]\n",
    "\\\\ \\vdots\n",
    "\\\\ [center\\,word\\,vector\\,n] \\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We multiply $U$ by a center word $v_{4}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\begin{align*}\n",
    "U \\cdot v_{4} &= \\begin{bmatrix}[outside\\,word\\,vector\\,1]\n",
    "\\\\ \\vdots\n",
    "\\\\ [outside\\,word\\,vector\\,n] \\end{bmatrix}\n",
    "\\cdot v_{4}\n",
    "\\\\ &=\\begin{bmatrix}[similarity\\,(u_{1}, v_{4})]\n",
    "\\\\ \\vdots\n",
    "\\\\ [similarity\\,(u_{n}, v_{4})] \\end{bmatrix}\n",
    "\\end{align*}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We apply softmax to get a vector of probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$softmax(U\\cdot u_{4})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remark 1\n",
    "**The outside words that are predicted will always be the same !!**"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "House house [center word] house house\n",
    "if P(house|center word) = max P(context word|center word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means that our model will give a reasonably high probability estimate to all words that occur in the context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remark 2\n",
    "The words 'and', 'the', 'of', ... will have very high frequency with all the other words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remark 3\n",
    "The 2D-projections of the word clouds are very misleading  \n",
    "In very high dimensional space, a word can be close to lots of other words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "## Optimization: Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Objective:**\n",
    "* Minimise cost function $J(\\theta)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Idea:** \n",
    "* For current value of $\\theta$, calculate the gradient of $J(\\theta)$\n",
    "* Take a small step in the direction of negative gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\theta^{new} = \\theta^{old} - \\alpha \\nabla_{\\theta}J(\\theta) $$ where $\\alpha$ is the learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "### Stochastic Gradient Decsent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* $J(\\theta)$ is a function of all windows in the corpus!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution:**\n",
    "\n",
    "Stochastic Gradient Descent !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Repeatedly sample windows\n",
    "* Update our parameters after going through each window\n",
    "* The parameters are updated with amazingly noisy gradient, but it doesn't matter too much \n",
    "* It allows us to go much quicker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remark 1\n",
    "\n",
    "Choose mini-batch size of 32, 64, or other powers of 2, as they allow to make the most out of parallelization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remark 2\n",
    "\n",
    "* In each window, we have a certain number of words $2m+1$\n",
    "* Our parameter vector $\\theta$ is in $\\mathbb{R}^{2dV}$, which is much bigger\n",
    "* Hence, $\\nabla_{\\theta} J(\\theta)$ is a very sparce matrix\n",
    "\n",
    "=> **Idea:**\n",
    "\n",
    "Only update the word vectors that appear in our window!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "### Why 2 vectors for each word?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* It is easy to optimize\n",
    "* We just average them to get a unique word vector in the end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2 Model variants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Skip-gram: Predict outside words (position independent) given center word\n",
    "2. Continuous Bag of Words: predict center word from a Bag of context words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results we get via these 2 methods are quite similar, as the dot product is symmetric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Negative Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Idea:**\n",
    "\n",
    "Train binary logistic regression for a true pair (center word & word in its context window)  \n",
    "VS  \n",
    "Several noise pairs (center word & a random word) = Negative samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Maximize probability that real outside word appears\n",
    "* Minimize probability that random words appear around center word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We sample these words using the Unigram distribution to determine their probability of being sampled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$P(w) = U(w)^{3/4} / Z$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The 3/4 power helps to make the most frequent words appear more often, and the less frequent words appear more\n",
    "* $Z$ is a normalization factor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remark:**\n",
    "\n",
    "The sigmoid function is like a binary case for the softmax function: maps our values to a probability distribution in [0,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "## Count-based methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could also count co-occurences of words within a certain window...  \n",
    "Why don't we just do that?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could create a matrix of co-occurrences  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Co-Occurences matrix](images\\Lecture02_cooccurences_matrix_boxes.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2 words would be 'similar' if their co-occurence vectors are similar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reduced SVD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Singular Value Decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ X = U * \\Sigma * V^{T} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* $\\Sigma$ is a diagonal matrix (can be rectangular if X not square)  \n",
    "Its diagonal values are in decreasing order\n",
    "* $U$, $V$ are square matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reduced version:**  \n",
    "We remove the dimensions with the lowest diagonal values for $\\Sigma$  \n",
    "The corresponding column of $U$, and line of $V^{T}$ are removed as well  \n",
    "We then have $k$ dimensions left"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We the get the best rank $k$ approximation to X, in terms of least squares"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improvement ideas (*Rohde, 2005*)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Windows that give more importance to closer words:  \n",
    "Same as in word2vec, we sample closer words more often"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison of the 2 methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images\\Lecture02_CountBased-VS-DirectPred.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "## GloVe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General Idea"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**You want to have components of meaning to be linear operations in a vector space !**  \n",
    "In particular, ratios of co-occurrence probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to have dot-product to be equal to log(Probability)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ w_{i} \\cdot w_{j} = log P(i|j) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "so that:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ w_{x} \\cdot (w_{a} - w_{b}) = log \\frac {P(x|a)} {P(x|b)} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, \n",
    "* The objetive function is the squared difference between the dot-product and the log of the co-occurence probabilities  \n",
    "* It is complexified a bit by adding bias terms for both words\n",
    "* The $f$ function is used to limit the influence of very common words pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ J = \\sum_{i, j=1}^V f(X_{ij}) \\, (w_{i}^{T} \\tilde{w_{j}} + b_{i} + \\tilde{b_{j}} - log X_{ij})^2 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating word vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intrinsic VS Extrinsic evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intrinsic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Evaluation on a specific / intermediate task\n",
    "* Fast to compute"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extrinsic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Evaluation on a real task that humans like to use  \n",
    "(web search, question answering, phone dialogue system...)\n",
    "* Can take a long time to compute accuracy\n",
    "* Unclear where the problem is:\n",
    "    * The subsystem?\n",
    "    * Its interaction with other subsystems?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### On the Dimensionality of Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There seems to be a blip at 200-300, for the dimension of the embeddings, that seems to optimize performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The performance stays flat, even when we continue to increase dimensionality up to infinity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Wikipedia data seems to work better than a news corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Senses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Words can have lots of meanings!  \n",
    "(especially common words, and words that have existed for a long time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First simple idea"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Cluster word windows around words: 1 word vector for each sense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Different senses using linear superposition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Different senses of a word reside in a weighted sum of the vectors of each sense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ v_{pike} = \\alpha_{1} v_{pike_{1}} + \\alpha_{2} v_{pike_{2}} + \\alpha_{3} v_{pike_{3}} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where:\n",
    "$$ \\begin{align*}\n",
    " \\alpha_{1} &= \\frac{f_{1}}{f_{1}+f_{2}+f_{3}}\n",
    "\\\\ f_{i} &= frequency\\,(word\\,i)\n",
    "\\end{align*} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we have so many dimensions, and as the words vectors for each sense are sparse, the results we get are quite good !!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, we can get back the vectors of the senses using the weighted summed vector !!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LaTeX templates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$U = \\begin{bmatrix}[outside\\,word\\,vector\\,1]\n",
    "\\\\ \\vdots\n",
    "\\\\ [outside\\,word\\,vector\\,n] \\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$V = \\begin{bmatrix}[center\\,word\\,vector\\,1]\n",
    "\\\\ \\vdots\n",
    "\\\\ [center\\,word\\,vector\\,n] \\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We multiply $U$ by a center word $v_{4}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\begin{align*}\n",
    "U \\cdot v_{4} &= \\begin{bmatrix}[outside\\,word\\,vector\\,1]\n",
    "\\\\ \\vdots\n",
    "\\\\ [outside\\,word\\,vector\\,n] \\end{bmatrix}\n",
    "\\cdot v_{4}\n",
    "\\\\ &=\\begin{bmatrix}[similarity\\,(u_{1}, v_{4})]\n",
    "\\\\ \\vdots\n",
    "\\\\ [similarity\\,(u_{n}, v_{4})] \\end{bmatrix}\n",
    "\\end{align*}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We apply softmax to get a vector of probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\theta^{new} = \\theta^{old} - \\alpha \\nabla_{\\theta}J(\\theta) $$ where $\\alpha$ is the learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ X = U * \\Sigma * V^{T} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$P(w) = U(w)^{3/4} / Z$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ w_{i} \\cdot w_{j} = log P(i|j) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "so that:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ w_{x} \\cdot (w_{a} - w_{b}) = log \\frac {P(x|a)} {P(x|b)} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$softmax(U\\cdot u_{4})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ J = \\sum_{i, j=1}^V f(X_{ij}) \\, (w_{i}^{T} \\tilde{w_{j}} + b_{i} + \\tilde{b_{j}} - log X_{ij})^2 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ v_{pike} = \\alpha_{1} v_{pike_{1}} + \\alpha_{2} v_{pike_{2}} + \\alpha_{3} v_{pike_{3}} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where:\n",
    "$$ \\begin{align*}\n",
    " \\alpha_{1} &= \\frac{f_{1}}{f_{1}+f_{2}+f_{3}}\n",
    "\\\\ f_{i} &= frequency\\,(word\\,i)\n",
    "\\end{align*} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
